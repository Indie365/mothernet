{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226373c7-4c29-470b-8c06-d793716f4719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amueller-tabpfn-4gpu\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70decb3d-938c-4459-9f04-4f94c7d06682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd35cb69-a8d3-4125-b27a-21f60415aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9182b3ce-cd53-4237-83bb-894bd4ed7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401fdd35-5908-48c4-8e15-eb3a4b48ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn.scripts.distill_mlp import TorchMLP, DistilledTabPFNMLP\n",
    "from tabpfn.scripts.transformer_prediction_interface import TabPFNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c39630-be38-4681-b545-e860f9481653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TorchMLP(n_epochs=100).fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453b473f-9fe7-4a69-8118-ad0cfd5e94ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a9b9f3-0c7c-4a19-a9a5-5b476f784238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6c7a963-21ba-426f-af33-481410fc4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 149\n"
     ]
    }
   ],
   "source": [
    "from tabpfn.datasets import load_openml_list, open_cc_dids, open_cc_valid_dids, test_dids_classification\n",
    "\n",
    "cc_valid_datasets_multiclass, cc_valid_datasets_multiclass_df = load_openml_list(open_cc_valid_dids, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = 10000, num_feats=100, return_capped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e82f2db-a1b4-4b26-9a9d-20a1044614d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def make_mlp(categorical_features):\n",
    "    cont_pipe = make_pipeline(StandardScaler(), SimpleImputer())\n",
    "    preprocess = make_column_transformer((OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features), remainder=cont_pipe)\n",
    "    return make_pipeline(preprocess, TorchMLP(n_epochs=100))\n",
    "\n",
    "def make_distilled_tabpfn(categorical_features):\n",
    "    cont_pipe = make_pipeline(StandardScaler(), SimpleImputer())\n",
    "    preprocess = make_column_transformer((OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=10), categorical_features), remainder=cont_pipe)\n",
    "    return make_pipeline(preprocess, DistilledTabPFNMLP(n_epochs=100, device=\"cuda\"))\n",
    "\n",
    "def make_distilled_tabpfn_ht(categorical_features):\n",
    "    cont_pipe = make_pipeline(StandardScaler(), SimpleImputer())\n",
    "    preprocess = make_column_transformer((OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=10), categorical_features), remainder=cont_pipe)\n",
    "    return make_pipeline(preprocess, DistilledTabPFNMLP(n_epochs=100, temperature=10, device=\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23fe505a-b909-4662-9b84-c37d1a191214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn.scripts.evaluate_baselines_sklearn import make_logreg, make_knn, make_hgb, make_rf, make_tabpfn, make_mlp, make_distilled_tabpfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2afa776-957f-4a77-91bc-b943169a5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'mlp': make_mlp,\n",
    "          'distilled_tabpfn': make_distilled_tabpfn,\n",
    "         'distilled_tabpfn_ht': make_distilled_tabpfn_ht,\n",
    "\n",
    "          'logreg': make_logreg,\n",
    "            'knn': make_knn,\n",
    "            'rf': make_rf,\n",
    "            'tabpfn': make_tabpfn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c38070b2-9b08-4cb5-983d-ed4cb3ae0a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e7bc371-5658-4306-be96-286cb92fdf7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                 | 0/149 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast-cancer\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                                                                                                                                     | 1/149 [00:33<1:22:42, 33.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colic\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███                                                                                                                                                                                                                                    | 2/149 [01:08<1:24:20, 34.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dermatology\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████▋                                                                                                                                                                                                                                  | 3/149 [01:25<1:03:51, 26.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "sonar\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██████▎                                                                                                                                                                                                                                  | 4/149 [01:45<57:24, 23.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glass\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███████▊                                                                                                                                                                                                                                 | 5/149 [02:03<52:33, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haberman\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████████▍                                                                                                                                                                                                                               | 6/149 [02:26<53:06, 22.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tae\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████████▉                                                                                                                                                                                                                              | 7/149 [02:45<49:58, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart-c\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████████▌                                                                                                                                                                                                                            | 8/149 [03:11<53:33, 22.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart-h\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██████████████                                                                                                                                                                                                                           | 9/149 [03:39<56:39, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart-statlog\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████████████▌                                                                                                                                                                                                                        | 10/149 [04:04<56:35, 24.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hepatitis\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████████████▏                                                                                                                                                                                                                      | 11/149 [04:23<52:38, 22.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vote\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████████████▋                                                                                                                                                                                                                     | 12/149 [04:53<57:27, 25.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ionosphere\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████████████▏                                                                                                                                                                                                                   | 13/149 [05:17<56:12, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|█████████████████████▊                                                                                                                                                                                                                  | 14/149 [05:34<50:34, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████████████████▎                                                                                                                                                                                                                | 15/149 [05:54<48:01, 21.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hayes-roth\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████████████████▉                                                                                                                                                                                                               | 16/149 [06:12<45:31, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monks-problems-1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████████████████████▍                                                                                                                                                                                                             | 17/149 [06:49<56:19, 25.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monks-problems-2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████████████████████▊                                                                                                                                                                                                          | 18/149 [07:30<1:05:47, 30.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monks-problems-3\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████████████████████████▎                                                                                                                                                                                                        | 19/149 [08:07<1:09:26, 32.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPECT\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████████████████████▊                                                                                                                                                                                                       | 20/149 [08:32<1:04:26, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPECTF\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████████████████████▍                                                                                                                                                                                                     | 21/149 [09:00<1:03:11, 29.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grub-damage\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████████████████████████▎                                                                                                                                                                                                     | 22/149 [09:19<55:46, 26.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_control\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████████████████████████████▌                                                                                                                                                                                                  | 23/149 [10:01<1:05:08, 31.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prnn_crabs\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████████████████████████████▎                                                                                                                                                                                                  | 24/149 [10:18<55:45, 26.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_lawsuit\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████████████████████████████▉                                                                                                                                                                                                 | 25/149 [10:41<52:52, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irish\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████████████████████████████████▍                                                                                                                                                                                               | 26/149 [11:18<59:43, 29.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_broadwaymult\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████████████████████████████████                                                                                                                                                                                              | 27/149 [11:43<56:46, 27.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_reviewer\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████████████████████████▌                                                                                                                                                                                            | 28/149 [12:12<56:40, 28.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backache\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████████████████████████████████████▏                                                                                                                                                                                          | 29/149 [12:32<51:31, 25.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prnn_synth\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████████████████▋                                                                                                                                                                                         | 30/149 [12:48<45:10, 22.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schizo\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████████████████████████████████████▎                                                                                                                                                                                       | 31/149 [13:16<48:08, 24.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profb\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████████████████████████████████████▊                                                                                                                                                                                      | 32/149 [13:58<57:29, 29.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_germangss\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████████████████████████████████████▍                                                                                                                                                                                    | 33/149 [14:27<56:52, 29.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomed\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████████████████████████████████▉                                                                                                                                                                                   | 34/149 [14:50<52:45, 27.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmftsa_sleepdata\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████████████████████████████████████████                                                                                                                                                                                | 35/149 [15:38<1:04:04, 33.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diggle_table_a2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████████████████████████████████████████                                                                                                                                                                                | 36/149 [16:05<59:54, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmftsa_ladata\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████████████████████████████████                                                                                                                                                                             | 37/149 [16:44<1:03:14, 33.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwLinear\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████████████████████████████████████████████▏                                                                                                                                                                            | 38/149 [17:04<55:03, 29.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "analcatdata_vineyard\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████████████████████████████████████████████▋                                                                                                                                                                           | 39/149 [17:38<56:32, 30.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_cpu\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████████████████████████████████████████████▎                                                                                                                                                                         | 40/149 [17:54<48:17, 26.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pharynx\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████████████████████████████████████████▊                                                                                                                                                                        | 41/149 [18:18<46:12, 25.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_price\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████████████████████████████████████████▍                                                                                                                                                                      | 42/149 [18:38<42:43, 23.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "servo\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████████████████████████████████████████████████▉                                                                                                                                                                     | 43/149 [18:55<38:53, 22.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_wildcat\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                   | 44/149 [19:16<37:37, 21.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm10\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████████████████████████████████                                                                                                                                                                  | 45/149 [19:48<42:59, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wisconsin\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████████████████████████████████████████████████████████████████▌                                                                                                                                                                | 46/149 [20:10<41:09, 23.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoPrice\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████████████████████████████████████████████▏                                                                                                                                                              | 47/149 [20:30<38:30, 22.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████████████████████████████████████████████████████▋                                                                                                                                                             | 48/149 [21:08<46:03, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_apnea3\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                           | 49/149 [21:40<48:05, 28.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_apnea2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                          | 50/149 [22:13<49:34, 30.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_apnea1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                        | 51/149 [22:51<52:52, 32.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disclosure_x_bias\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                       | 52/149 [23:20<50:48, 31.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodyfat\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                     | 53/149 [23:44<46:22, 28.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleveland\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                    | 54/149 [24:05<42:21, 26.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triazines\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                  | 55/149 [24:29<40:21, 25.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disclosure_x_tampered\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 56/149 [24:51<38:22, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                               | 57/149 [25:10<35:18, 23.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cholesterol\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                             | 58/149 [25:34<35:22, 23.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_funds\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                            | 59/149 [25:48<30:36, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbcseq\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                          | 60/149 [26:04<28:43, 19.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "pbc\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                         | 61/149 [26:37<34:04, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmftsa_ctoarrivals\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                       | 62/149 [27:00<33:38, 23.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_vine2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                      | 63/149 [27:21<32:29, 22.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatfield_4\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                    | 64/149 [27:44<32:08, 22.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston_corrected\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                  | 65/149 [28:21<37:41, 26.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensory\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                 | 66/149 [29:01<42:49, 30.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disclosure_x_noise\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                               | 67/149 [29:27<40:13, 29.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoMpg\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                              | 68/149 [30:01<41:23, 30.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kdd_el_nino-small\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                            | 69/149 [30:52<49:00, 36.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "autoHorse\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                           | 70/149 [31:13<42:13, 32.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                         | 71/149 [32:14<53:01, 40.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breastTumor\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                        | 72/149 [32:39<46:13, 36.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_gsssexsurvey\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                      | 73/149 [32:58<39:08, 30.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                    | 74/149 [33:33<40:14, 32.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fishcatch\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                   | 75/149 [33:48<33:33, 27.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vinnie\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                 | 76/149 [34:07<29:53, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu284\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                | 77/149 [34:31<29:19, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                              | 78/149 [35:03<31:36, 26.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_geyser1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                             | 79/149 [35:19<27:15, 23.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census6\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                           | 80/149 [35:46<28:08, 24.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census5\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                          | 81/149 [36:14<28:58, 25.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census4\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                        | 82/149 [36:43<29:44, 26.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census3\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                      | 83/149 [37:12<30:09, 27.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                     | 84/149 [37:41<30:19, 27.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plasma_retinol\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                   | 85/149 [38:11<30:21, 28.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualizing_galaxy\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                  | 86/149 [38:30<26:47, 25.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colleges_usnews\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                | 87/149 [38:47<23:42, 22.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "disclosure_z\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                               | 88/149 [39:13<24:29, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socmob\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 89/149 [40:31<40:07, 40.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_whale\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 90/149 [40:50<33:18, 33.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water-treatment\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                          | 91/149 [41:07<27:48, 28.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "lowbwt\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                        | 92/149 [41:27<24:47, 26.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arsenic-female-bladder\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/fresh311/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                       | 93/149 [42:02<26:56, 28.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_halloffame\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                     | 94/149 [42:15<21:56, 23.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "analcatdata_birthday\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                    | 95/149 [42:42<22:26, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_draft\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                  | 96/149 [43:10<22:57, 25.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collins\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                 | 97/149 [43:47<25:14, 29.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prnn_fglass\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                               | 98/149 [44:09<22:59, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jEdit_4.2_4.3\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                             | 99/149 [44:40<23:28, 28.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                            | 100/149 [44:57<20:12, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mw1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                          | 101/149 [45:29<21:36, 27.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jEdit_4.0_4.2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                        | 102/149 [45:56<21:05, 26.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PopularKids\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                       | 103/149 [46:30<22:22, 29.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teachingAssistant\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                     | 104/149 [46:49<19:36, 26.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lungcancer_GSE31210\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                    | 105/149 [47:11<18:19, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MegaWatt1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                  | 106/149 [47:35<17:36, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PizzaCutter1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                 | 107/149 [48:19<21:11, 30.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PizzaCutter3\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                               | 108/149 [49:27<28:31, 41.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CostaMadre1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                              | 109/149 [49:48<23:40, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CastMetal1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                            | 110/149 [50:19<22:09, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnuggetChase3\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                           | 111/149 [50:40<19:11, 30.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PieChart1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 112/149 [51:28<21:51, 35.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PieChart3\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                       | 113/149 [52:39<27:40, 46.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "parkinsons\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 114/149 [53:01<22:43, 38.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planning-relax\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                    | 115/149 [53:25<19:36, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qualitative-bankruptcy\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                   | 116/149 [53:48<17:04, 31.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa-heart\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                 | 117/149 [54:23<17:11, 32.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeds\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                | 118/149 [54:41<14:25, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thoracic-surgery\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                              | 119/149 [55:14<14:46, 29.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user-knowledge\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                             | 120/149 [55:43<14:06, 29.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wholesale-customers\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                           | 121/149 [56:13<13:45, 29.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart-long-beach\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 122/149 [56:35<12:13, 27.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot-failures-lp5\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                        | 123/149 [56:55<10:54, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertebra-column\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                      | 124/149 [57:20<10:27, 25.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartphone-Based_Recognition_of_Human_Activities\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                     | 125/149 [57:41<09:35, 23.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast-cancer-dropped-missing-attributes-values\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                   | 126/149 [58:04<09:03, 23.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LED-display-domain-7digit\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 127/149 [58:39<09:49, 26.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMETES_Epistasis_2-Way_20atts_0.1H_EDM-1_1\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 128/149 [58:53<08:07, 23.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "calendarDOW\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 129/149 [59:07<06:46, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 197, in fit\n",
      "    raise ValueError(\"The number of features for this classifier is restricted to \", self.max_num_features)\n",
      "ValueError: ('The number of features for this classifier is restricted to ', 100)\n",
      "\n",
      "corral\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 130/149 [59:26<06:19, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mofn-3-7-10\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/distill_mlp.py\", line 94, in fit\n",
      "    tbfn = TabPFNClassifier(N_ensemble_configurations=32, temperature=self.temperature, device=self.device).fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 131/149 [59:38<05:17, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda/envs/fresh311/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/azureuser/TabPFN/tabpfn/scripts/transformer_prediction_interface.py\", line 201, in fit\n",
      "    raise ValueError(\"⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\")\n",
      "ValueError: ⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "\n",
      "thyroid-new\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                          | 132/149 [59:59<05:17, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solar-flare\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 133/149 [1:00:25<05:34, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threeOf9\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                       | 134/149 [1:01:00<06:16, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xd6\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 135/149 [1:02:03<08:28, 36.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokyo1\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 136/149 [1:03:09<09:46, 45.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity5_plus_5\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 137/149 [1:04:25<10:53, 54.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleve\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 138/149 [1:04:53<08:33, 46.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleveland-nominal\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 139/149 [1:05:19<06:42, 40.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australian\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 140/149 [1:06:08<06:26, 42.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiabeticMellitus\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 141/149 [1:06:32<04:57, 37.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conference_attendance\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 142/149 [1:06:54<03:49, 32.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPMP-2015-runtime-classification\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 143/149 [1:07:31<03:24, 34.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TuningSVMs\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 144/149 [1:07:51<02:28, 29.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regime_alimentaire\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 145/149 [1:08:15<01:51, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris-example\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 146/149 [1:08:30<01:13, 24.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Touch2\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 147/149 [1:08:53<00:47, 23.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penguins\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 148/149 [1:09:22<00:25, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 149/149 [1:10:20<00:00, 28.33s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "all_scores = defaultdict(dict)\n",
    "for ds_name, X, y, categorical_features, _, _ in tqdm(cc_valid_datasets_multiclass):\n",
    "    print(ds_name)\n",
    "    for model_name, model_creator in models.items():\n",
    "        clf = model_creator(categorical_features)\n",
    "        if X.shape[1] > 100:\n",
    "            X = X[:, :100]\n",
    "        try:\n",
    "            scores = cross_validate(clf, X, y, scoring=\"roc_auc_ovo\", n_jobs=-1)\n",
    "            score = scores['test_score'].mean()\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            score = np.NaN\n",
    "        all_scores[ds_name][model_name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c88a13b4-b764-42a2-9217-801ea3fe3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame.from_dict(all_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05b3e604-5938-4271-916d-e5bb44e513f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp                    0\n",
       "distilled_tabpfn       8\n",
       "distilled_tabpfn_ht    8\n",
       "logreg                 0\n",
       "knn                    0\n",
       "rf                     0\n",
       "tabpfn                 8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcd712d4-f6f2-47ce-8139-df8922f6d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"sklearn_implementation_baselines_auc_redo.cvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c1cc13c-944f-4a1a-9b5e-8cb510da0baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mlp</th>\n",
       "      <th>distilled_tabpfn</th>\n",
       "      <th>distilled_tabpfn_ht</th>\n",
       "      <th>logreg</th>\n",
       "      <th>knn</th>\n",
       "      <th>rf</th>\n",
       "      <th>tabpfn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>breast-cancer</th>\n",
       "      <td>0.705165</td>\n",
       "      <td>0.719032</td>\n",
       "      <td>0.718443</td>\n",
       "      <td>0.694928</td>\n",
       "      <td>0.637934</td>\n",
       "      <td>0.683816</td>\n",
       "      <td>0.722461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colic</th>\n",
       "      <td>0.885905</td>\n",
       "      <td>0.885719</td>\n",
       "      <td>0.885253</td>\n",
       "      <td>0.875698</td>\n",
       "      <td>0.847037</td>\n",
       "      <td>0.913075</td>\n",
       "      <td>0.894787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sonar</th>\n",
       "      <td>0.934220</td>\n",
       "      <td>0.947244</td>\n",
       "      <td>0.947698</td>\n",
       "      <td>0.870630</td>\n",
       "      <td>0.897559</td>\n",
       "      <td>0.928207</td>\n",
       "      <td>0.947794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass</th>\n",
       "      <td>0.903652</td>\n",
       "      <td>0.905528</td>\n",
       "      <td>0.905942</td>\n",
       "      <td>0.881423</td>\n",
       "      <td>0.878902</td>\n",
       "      <td>0.944137</td>\n",
       "      <td>0.938821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haberman</th>\n",
       "      <td>0.673186</td>\n",
       "      <td>0.723971</td>\n",
       "      <td>0.723954</td>\n",
       "      <td>0.662810</td>\n",
       "      <td>0.645131</td>\n",
       "      <td>0.684003</td>\n",
       "      <td>0.711667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regime_alimentaire</th>\n",
       "      <td>0.949590</td>\n",
       "      <td>0.945944</td>\n",
       "      <td>0.945944</td>\n",
       "      <td>0.961458</td>\n",
       "      <td>0.901349</td>\n",
       "      <td>0.955579</td>\n",
       "      <td>0.927367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iris-example</th>\n",
       "      <td>0.995333</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.997333</td>\n",
       "      <td>0.991500</td>\n",
       "      <td>0.995333</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Touch2</th>\n",
       "      <td>0.973185</td>\n",
       "      <td>0.981328</td>\n",
       "      <td>0.981394</td>\n",
       "      <td>0.952478</td>\n",
       "      <td>0.949996</td>\n",
       "      <td>0.979879</td>\n",
       "      <td>0.988497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penguins</th>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.999152</td>\n",
       "      <td>0.999797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titanic</th>\n",
       "      <td>0.828302</td>\n",
       "      <td>0.866233</td>\n",
       "      <td>0.866366</td>\n",
       "      <td>0.854819</td>\n",
       "      <td>0.850946</td>\n",
       "      <td>0.866552</td>\n",
       "      <td>0.864852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mlp  distilled_tabpfn  distilled_tabpfn_ht    logreg  \\\n",
       "breast-cancer       0.705165          0.719032             0.718443  0.694928   \n",
       "colic               0.885905          0.885719             0.885253  0.875698   \n",
       "sonar               0.934220          0.947244             0.947698  0.870630   \n",
       "glass               0.903652          0.905528             0.905942  0.881423   \n",
       "haberman            0.673186          0.723971             0.723954  0.662810   \n",
       "...                      ...               ...                  ...       ...   \n",
       "regime_alimentaire  0.949590          0.945944             0.945944  0.961458   \n",
       "iris-example        0.995333          0.997000             0.997000  0.997333   \n",
       "Touch2              0.973185          0.981328             0.981394  0.952478   \n",
       "penguins            0.999797          0.999797             0.999797  0.999797   \n",
       "titanic             0.828302          0.866233             0.866366  0.854819   \n",
       "\n",
       "                         knn        rf    tabpfn  \n",
       "breast-cancer       0.637934  0.683816  0.722461  \n",
       "colic               0.847037  0.913075  0.894787  \n",
       "sonar               0.897559  0.928207  0.947794  \n",
       "glass               0.878902  0.944137  0.938821  \n",
       "haberman            0.645131  0.684003  0.711667  \n",
       "...                      ...       ...       ...  \n",
       "regime_alimentaire  0.901349  0.955579  0.927367  \n",
       "iris-example        0.991500  0.995333  0.997000  \n",
       "Touch2              0.949996  0.979879  0.988497  \n",
       "penguins            0.999638  0.999152  0.999797  \n",
       "titanic             0.850946  0.866552  0.864852  \n",
       "\n",
       "[141 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efc2554e-9881-4d82-97fa-37987f92f75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>classifier_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breast-cancer</td>\n",
       "      <td>mlp</td>\n",
       "      <td>0.705165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>breast-cancer</td>\n",
       "      <td>distilled_tabpfn</td>\n",
       "      <td>0.719032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>breast-cancer</td>\n",
       "      <td>distilled_tabpfn_ht</td>\n",
       "      <td>0.718443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breast-cancer</td>\n",
       "      <td>logreg</td>\n",
       "      <td>0.694928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>breast-cancer</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.637934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>titanic</td>\n",
       "      <td>distilled_tabpfn_ht</td>\n",
       "      <td>0.866366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>titanic</td>\n",
       "      <td>logreg</td>\n",
       "      <td>0.854819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>titanic</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.850946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>titanic</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.866552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>titanic</td>\n",
       "      <td>tabpfn</td>\n",
       "      <td>0.864852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>987 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset_name      classifier_name  accuracy\n",
       "0    breast-cancer                  mlp  0.705165\n",
       "1    breast-cancer     distilled_tabpfn  0.719032\n",
       "2    breast-cancer  distilled_tabpfn_ht  0.718443\n",
       "3    breast-cancer               logreg  0.694928\n",
       "4    breast-cancer                  knn  0.637934\n",
       "..             ...                  ...       ...\n",
       "982        titanic  distilled_tabpfn_ht  0.866366\n",
       "983        titanic               logreg  0.854819\n",
       "984        titanic                  knn  0.850946\n",
       "985        titanic                   rf  0.866552\n",
       "986        titanic               tabpfn  0.864852\n",
       "\n",
       "[987 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla  = results.dropna().stack().reset_index()\n",
    "bla.columns = ['dataset_name', 'classifier_name', 'accuracy']\n",
    "bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492b8c1-7136-4a2b-b1da-a7595a97ece3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "958b3ca2-841a-4cdd-995f-ec964301ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mlp' 'distilled_tabpfn' 'distilled_tabpfn_ht' 'logreg' 'knn' 'rf'\n",
      " 'tabpfn']\n",
      "distilled_tabpfn        5.0\n",
      "distilled_tabpfn_ht     5.0\n",
      "knn                     2.0\n",
      "logreg                 24.0\n",
      "mlp                    15.0\n",
      "rf                     31.0\n",
      "tabpfn                 38.0\n",
      "dtype: float64\n",
      "knn                    6.014184\n",
      "mlp                    4.443262\n",
      "logreg                 4.191489\n",
      "rf                     3.762411\n",
      "distilled_tabpfn       3.432624\n",
      "distilled_tabpfn_ht    3.418440\n",
      "tabpfn                 2.737589\n",
      "dtype: float64\n",
      "('knn', 'tabpfn', 1.1568307081455343e-18, True)\n",
      "('distilled_tabpfn_ht', 'knn', 2.0717995978850566e-16, True)\n",
      "('distilled_tabpfn', 'knn', 2.2572501479523486e-16, True)\n",
      "('knn', 'rf', 6.370377301692491e-16, True)\n",
      "('knn', 'mlp', 1.2135631770967177e-10, True)\n",
      "('mlp', 'tabpfn', 1.7342425414048873e-08, True)\n",
      "('logreg', 'tabpfn', 3.161740467328801e-08, True)\n",
      "('knn', 'logreg', 6.977317169530739e-07, True)\n",
      "('distilled_tabpfn_ht', 'mlp', 1.855626359200013e-06, True)\n",
      "('distilled_tabpfn', 'mlp', 1.8939242095882943e-06, True)\n",
      "('distilled_tabpfn_ht', 'logreg', 2.3974447890667034e-05, True)\n",
      "('distilled_tabpfn', 'logreg', 2.824194459123757e-05, True)\n",
      "('distilled_tabpfn', 'tabpfn', 4.2859827746873124e-05, True)\n",
      "('distilled_tabpfn_ht', 'tabpfn', 4.440788529589581e-05, True)\n",
      "('rf', 'tabpfn', 0.0012810679552422988, True)\n",
      "('mlp', 'rf', 0.004075734494382588, True)\n",
      "('logreg', 'rf', 0.02180607716871586, False)\n",
      "('distilled_tabpfn_ht', 'rf', 0.12218893360547221, False)\n",
      "('distilled_tabpfn', 'rf', 0.12672927146510865, False)\n",
      "('distilled_tabpfn', 'distilled_tabpfn_ht', 0.5477942485661857, False)\n",
      "('logreg', 'mlp', 0.8603116334957511, False)\n",
      "Index(['knn', 'mlp', 'logreg', 'rf', 'distilled_tabpfn', 'distilled_tabpfn_ht',\n",
      "       'tabpfn'],\n",
      "      dtype='object')\n",
      "[1, 2]\n",
      "[3, 2]\n",
      "[3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x185 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tabpfn.scripts.critical_differences import draw_cd_diagram\n",
    "res = draw_cd_diagram(bla, title=\"OVO ROC AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12646f88-f70d-4cb1-81a2-c9e1c850cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_csv(\"sklearn_implementation_baselines_auc_redo.cvs\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c08a5a-df86-455a-b3f9-a6db385df6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc3ae28-ac75-4789-adef-def8b503aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lgbm(categorical_features):\n",
    "    return LGBMClassifier(categorical_features=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bdf6a3b-98a2-4c88-bdad-953734528bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mlp_shallow(categorical_features):\n",
    "    cont_pipe = make_pipeline(StandardScaler(), SimpleImputer())\n",
    "    preprocess = make_column_transformer((OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features), remainder=cont_pipe)\n",
    "    return make_pipeline(preprocess, TorchMLP(n_epochs=100, hidden_size=128, n_layers=1))\n",
    "\n",
    "def make_distilled_tabpfn_shallow(categorical_features):\n",
    "    cont_pipe = make_pipeline(StandardScaler(), SimpleImputer())\n",
    "    preprocess = make_column_transformer((OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=10), categorical_features), remainder=cont_pipe)\n",
    "    return make_pipeline(preprocess, DistilledTabPFNMLP(n_epochs=100, device=\"cuda\", hidden_size=128, n_layers=1))\n",
    "\n",
    "def make_mlp_big(categorical_features):\n",
    "    cont_pipe = make_pipeline(StandardScaler(), SimpleImputer())\n",
    "    preprocess = make_column_transformer((OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features), remainder=cont_pipe)\n",
    "    return make_pipeline(preprocess, TorchMLP(n_epochs=100, hidden_size=1024, n_layers=4))\n",
    "\n",
    "def make_distilled_tabpfn_big(categorical_features):\n",
    "    cont_pipe = make_pipeline(StandardScaler(), SimpleImputer())\n",
    "    preprocess = make_column_transformer((OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=10), categorical_features), remainder=cont_pipe)\n",
    "    return make_pipeline(preprocess, DistilledTabPFNMLP(n_epochs=100, device=\"cuda\", hidden_size=1024, n_layers=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4963243d-1df7-4621-8dfe-bfd185a7a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'mlp': make_mlp,\n",
    "          'mlp_shallow': make_mlp_shallow,\n",
    "          'mlp_big': make_mlp_big,\n",
    "          'distilled_tabpfn': make_distilled_tabpfn,\n",
    "         'distilled_tabpfn_ht': make_distilled_tabpfn_ht,\n",
    "           'distilled_tabpfn_shallow': make_distilled_tabpfn_shallow,\n",
    "         'distilled_tabpfn_big': make_distilled_tabpfn_big,\n",
    "\n",
    "          'logreg': make_logreg,\n",
    "            'knn': make_knn,\n",
    "            'rf': make_rf,\n",
    "            'tabpfn': make_tabpfn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "806ba503-2b05-49d4-bd22-81aab2df83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if model not in results.columns:\n",
    "        results[model] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96d2a700-46f4-45c7-aa5c-5641f7b72ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mlp</th>\n",
       "      <th>distilled_tabpfn</th>\n",
       "      <th>distilled_tabpfn_ht</th>\n",
       "      <th>logreg</th>\n",
       "      <th>knn</th>\n",
       "      <th>rf</th>\n",
       "      <th>tabpfn</th>\n",
       "      <th>mlp_shallow</th>\n",
       "      <th>mlp_big</th>\n",
       "      <th>distilled_tabpfn_shallow</th>\n",
       "      <th>distilled_tabpfn_big</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>breast-cancer</th>\n",
       "      <td>0.705165</td>\n",
       "      <td>0.719032</td>\n",
       "      <td>0.718443</td>\n",
       "      <td>0.694928</td>\n",
       "      <td>0.637934</td>\n",
       "      <td>0.683816</td>\n",
       "      <td>0.722461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colic</th>\n",
       "      <td>0.885905</td>\n",
       "      <td>0.885719</td>\n",
       "      <td>0.885253</td>\n",
       "      <td>0.875698</td>\n",
       "      <td>0.847037</td>\n",
       "      <td>0.913075</td>\n",
       "      <td>0.894787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dermatology</th>\n",
       "      <td>0.998480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998870</td>\n",
       "      <td>0.996039</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sonar</th>\n",
       "      <td>0.934220</td>\n",
       "      <td>0.947244</td>\n",
       "      <td>0.947698</td>\n",
       "      <td>0.870630</td>\n",
       "      <td>0.897559</td>\n",
       "      <td>0.928207</td>\n",
       "      <td>0.947794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass</th>\n",
       "      <td>0.903652</td>\n",
       "      <td>0.905528</td>\n",
       "      <td>0.905942</td>\n",
       "      <td>0.881423</td>\n",
       "      <td>0.878902</td>\n",
       "      <td>0.944137</td>\n",
       "      <td>0.938821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regime_alimentaire</th>\n",
       "      <td>0.949590</td>\n",
       "      <td>0.945944</td>\n",
       "      <td>0.945944</td>\n",
       "      <td>0.961458</td>\n",
       "      <td>0.901349</td>\n",
       "      <td>0.955579</td>\n",
       "      <td>0.927367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iris-example</th>\n",
       "      <td>0.995333</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.997333</td>\n",
       "      <td>0.991500</td>\n",
       "      <td>0.995333</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Touch2</th>\n",
       "      <td>0.973185</td>\n",
       "      <td>0.981328</td>\n",
       "      <td>0.981394</td>\n",
       "      <td>0.952478</td>\n",
       "      <td>0.949996</td>\n",
       "      <td>0.979879</td>\n",
       "      <td>0.988497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penguins</th>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.999152</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titanic</th>\n",
       "      <td>0.828302</td>\n",
       "      <td>0.866233</td>\n",
       "      <td>0.866366</td>\n",
       "      <td>0.854819</td>\n",
       "      <td>0.850946</td>\n",
       "      <td>0.866552</td>\n",
       "      <td>0.864852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mlp  distilled_tabpfn  distilled_tabpfn_ht    logreg  \\\n",
       "breast-cancer       0.705165          0.719032             0.718443  0.694928   \n",
       "colic               0.885905          0.885719             0.885253  0.875698   \n",
       "dermatology         0.998480               NaN                  NaN  0.998870   \n",
       "sonar               0.934220          0.947244             0.947698  0.870630   \n",
       "glass               0.903652          0.905528             0.905942  0.881423   \n",
       "...                      ...               ...                  ...       ...   \n",
       "regime_alimentaire  0.949590          0.945944             0.945944  0.961458   \n",
       "iris-example        0.995333          0.997000             0.997000  0.997333   \n",
       "Touch2              0.973185          0.981328             0.981394  0.952478   \n",
       "penguins            0.999797          0.999797             0.999797  0.999797   \n",
       "titanic             0.828302          0.866233             0.866366  0.854819   \n",
       "\n",
       "                         knn        rf    tabpfn  mlp_shallow  mlp_big  \\\n",
       "breast-cancer       0.637934  0.683816  0.722461          NaN      NaN   \n",
       "colic               0.847037  0.913075  0.894787          NaN      NaN   \n",
       "dermatology         0.996039  0.999330       NaN          NaN      NaN   \n",
       "sonar               0.897559  0.928207  0.947794          NaN      NaN   \n",
       "glass               0.878902  0.944137  0.938821          NaN      NaN   \n",
       "...                      ...       ...       ...          ...      ...   \n",
       "regime_alimentaire  0.901349  0.955579  0.927367          NaN      NaN   \n",
       "iris-example        0.991500  0.995333  0.997000          NaN      NaN   \n",
       "Touch2              0.949996  0.979879  0.988497          NaN      NaN   \n",
       "penguins            0.999638  0.999152  0.999797          NaN      NaN   \n",
       "titanic             0.850946  0.866552  0.864852          NaN      NaN   \n",
       "\n",
       "                    distilled_tabpfn_shallow  distilled_tabpfn_big  \n",
       "breast-cancer                            NaN                   NaN  \n",
       "colic                                    NaN                   NaN  \n",
       "dermatology                              NaN                   NaN  \n",
       "sonar                                    NaN                   NaN  \n",
       "glass                                    NaN                   NaN  \n",
       "...                                      ...                   ...  \n",
       "regime_alimentaire                       NaN                   NaN  \n",
       "iris-example                             NaN                   NaN  \n",
       "Touch2                                   NaN                   NaN  \n",
       "penguins                                 NaN                   NaN  \n",
       "titanic                                  NaN                   NaN  \n",
       "\n",
       "[149 rows x 11 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44670d50-84b0-4495-bb98-06e02714138a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                 | 0/149 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast-cancer\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                                                                                                                                     | 1/149 [00:28<1:09:46, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colic\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███                                                                                                                                                                                                                                    | 2/149 [00:59<1:13:07, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dermatology\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_ht\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_shallow\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_big\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "tabpfn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████▋                                                                                                                                                                                                                                  | 3/149 [01:50<1:36:48, 39.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of features for this classifier is restricted to ', 100)\n",
      "sonar\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██████▏                                                                                                                                                                                                                                | 4/149 [02:17<1:24:02, 34.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glass\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███████▊                                                                                                                                                                                                                               | 5/149 [02:43<1:15:14, 31.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haberman\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████████▎                                                                                                                                                                                                                             | 6/149 [03:12<1:13:03, 30.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tae\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████████▊                                                                                                                                                                                                                            | 7/149 [03:37<1:08:00, 28.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart-c\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████████▍                                                                                                                                                                                                                          | 8/149 [04:07<1:08:45, 29.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart-h\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████████████▉                                                                                                                                                                                                                         | 9/149 [04:39<1:09:56, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.10 GiB total capacity; 126.17 MiB already allocated; 60.56 MiB free; 134.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "heart-statlog\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████████████▍                                                                                                                                                                                                                      | 10/149 [05:07<1:08:24, 29.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hepatitis\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████████████████▉                                                                                                                                                                                                                     | 11/149 [05:31<1:03:33, 27.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vote\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████████████▌                                                                                                                                                                                                                   | 12/149 [06:03<1:06:20, 29.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ionosphere\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████████████                                                                                                                                                                                                                  | 13/149 [06:35<1:07:43, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|█████████████████████▌                                                                                                                                                                                                                | 14/149 [07:08<1:09:21, 30.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.10 GiB total capacity; 23.05 MiB already allocated; 54.56 MiB free; 24.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "wine\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████████████████▏                                                                                                                                                                                                              | 15/149 [07:34<1:05:39, 29.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hayes-roth\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████████████████▋                                                                                                                                                                                                             | 16/149 [07:57<1:01:06, 27.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monks-problems-1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████████████████████▏                                                                                                                                                                                                           | 17/149 [08:34<1:06:51, 30.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monks-problems-2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████████████████████▊                                                                                                                                                                                                          | 18/149 [09:13<1:11:46, 32.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monks-problems-3\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████████████████████████▎                                                                                                                                                                                                        | 19/149 [09:50<1:14:02, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "SPECT\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████████████████████▊                                                                                                                                                                                                       | 20/149 [10:19<1:09:57, 32.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPECTF\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████████████████████▍                                                                                                                                                                                                     | 21/149 [10:49<1:07:47, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grub-damage\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████████████████████▉                                                                                                                                                                                                    | 22/149 [11:13<1:02:22, 29.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_control\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████████████████████████████▌                                                                                                                                                                                                  | 23/149 [11:53<1:08:45, 32.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prnn_crabs\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████████████████████████████                                                                                                                                                                                                 | 24/149 [12:26<1:08:21, 32.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "analcatdata_lawsuit\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████████████████████████████▌                                                                                                                                                                                               | 25/149 [12:55<1:05:33, 31.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irish\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████████████████████████████████▏                                                                                                                                                                                             | 26/149 [13:30<1:07:07, 32.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_broadwaymult\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████████████████████████████████▋                                                                                                                                                                                            | 27/149 [13:59<1:03:55, 31.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_reviewer\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████████████████████████▏                                                                                                                                                                                          | 28/149 [14:31<1:03:48, 31.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backache\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████████████████████████████████▊                                                                                                                                                                                         | 29/149 [15:03<1:03:39, 31.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "prnn_synth\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████████████████▎                                                                                                                                                                                       | 30/149 [15:32<1:01:10, 30.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schizo\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████████████████████████████████████▎                                                                                                                                                                                       | 31/149 [16:01<59:44, 30.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profb\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████████████████████████████████████▍                                                                                                                                                                                    | 32/149 [16:43<1:05:50, 33.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_germangss\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████████████████████████████████████▉                                                                                                                                                                                   | 33/149 [17:16<1:04:57, 33.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomed\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████████████████████████████████▍                                                                                                                                                                                 | 34/149 [17:49<1:03:52, 33.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "rmftsa_sleepdata\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████████████████████████████████████████                                                                                                                                                                                | 35/149 [18:43<1:15:35, 39.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diggle_table_a2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████████████████████████████████████████▌                                                                                                                                                                              | 36/149 [19:12<1:08:35, 36.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmftsa_ladata\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████████████████████████████████                                                                                                                                                                             | 37/149 [19:48<1:07:58, 36.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwLinear\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████████████████████████████████████████████▋                                                                                                                                                                           | 38/149 [20:15<1:01:56, 33.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_vineyard\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 79.10 GiB total capacity; 333.00 MiB already allocated; 47.56 MiB free; 400.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████████████████████████████████████████████▏                                                                                                                                                                         | 39/149 [21:04<1:10:00, 38.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_cpu\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████████████████████████████████████████████▋                                                                                                                                                                        | 40/149 [21:29<1:01:52, 34.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pharynx\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████████████████████████████████████████▊                                                                                                                                                                        | 41/149 [21:54<56:25, 31.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_price\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████████████████████████████████████████▍                                                                                                                                                                      | 42/149 [22:18<52:12, 29.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "servo\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████████████████████████████████████████████████▉                                                                                                                                                                     | 43/149 [22:43<49:30, 28.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_wildcat\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                   | 44/149 [23:06<46:06, 26.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm10\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████████████████████████████████                                                                                                                                                                  | 45/149 [23:39<49:12, 28.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n",
      "wisconsin\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████████████████████████████████████████████████████████████████▌                                                                                                                                                                | 46/149 [24:05<47:36, 27.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoPrice\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████████████████████████████████████████████▏                                                                                                                                                              | 47/149 [24:28<44:35, 26.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████████████████████████████████████████████████████▋                                                                                                                                                             | 48/149 [25:05<49:38, 29.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_apnea3\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                           | 49/149 [25:40<52:09, 31.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_apnea2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                          | 50/149 [26:19<55:03, 33.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "analcatdata_apnea1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                        | 51/149 [26:55<56:01, 34.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disclosure_x_bias\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                       | 52/149 [27:35<58:01, 35.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodyfat\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                     | 53/149 [28:02<53:13, 33.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleveland\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                    | 54/149 [28:32<51:12, 32.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triazines\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                  | 55/149 [29:04<50:40, 32.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "disclosure_x_tampered\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 56/149 [29:46<54:41, 35.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                               | 57/149 [30:11<49:24, 32.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cholesterol\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                             | 58/149 [30:41<47:30, 31.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_funds\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                            | 59/149 [31:06<44:27, 29.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbcseq\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_ht\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_shallow\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_big\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "tabpfn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                         | 60/149 [32:38<1:11:34, 48.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "pbc\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                       | 61/149 [33:12<1:04:36, 44.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmftsa_ctoarrivals\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                       | 62/149 [33:40<56:43, 39.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_vine2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                      | 63/149 [34:15<54:18, 37.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatfield_4\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                    | 64/149 [34:42<49:15, 34.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston_corrected\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                  | 65/149 [35:21<50:20, 35.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "sensory\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                 | 66/149 [36:00<50:55, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disclosure_x_noise\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                               | 67/149 [36:40<51:39, 37.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoMpg\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                              | 68/149 [37:13<49:01, 36.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kdd_el_nino-small\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                            | 69/149 [37:59<52:28, 39.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoHorse\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                           | 70/149 [38:39<51:47, 39.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                         | 71/149 [39:30<55:44, 42.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breastTumor\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                        | 72/149 [39:58<49:31, 38.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_gsssexsurvey\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                      | 73/149 [40:23<43:21, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                    | 74/149 [41:00<43:57, 35.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fishcatch\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                   | 75/149 [41:35<43:17, 35.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vinnie\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                 | 76/149 [42:05<40:53, 33.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu284\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                | 77/149 [42:33<38:17, 31.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                              | 78/149 [43:09<39:08, 33.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_geyser1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                             | 79/149 [43:36<36:41, 31.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census6\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                           | 80/149 [44:13<38:00, 33.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n",
      "chscase_census5\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                          | 81/149 [44:47<37:44, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census4\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                        | 82/149 [45:18<36:29, 32.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census3\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                      | 83/149 [45:50<35:45, 32.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_census2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                     | 84/149 [46:23<35:23, 32.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plasma_retinol\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                   | 85/149 [46:58<35:28, 33.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "visualizing_galaxy\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                  | 86/149 [47:29<34:11, 32.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colleges_usnews\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_ht\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_shallow\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_big\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "tabpfn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                | 87/149 [48:36<44:28, 43.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "disclosure_z\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                               | 88/149 [49:19<43:30, 42.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socmob\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 89/149 [50:17<47:21, 47.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chscase_whale\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 90/149 [50:43<40:20, 41.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water-treatment\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_ht\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_shallow\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_big\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "tabpfn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                          | 91/149 [51:50<47:21, 48.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of features for this classifier is restricted to ', 100)\n",
      "lowbwt\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                        | 92/149 [52:17<40:15, 42.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arsenic-female-bladder\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                       | 93/149 [52:55<38:09, 40.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_halloffame\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_ht\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_shallow\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_big\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "tabpfn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                     | 94/149 [54:05<45:32, 49.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "analcatdata_birthday\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                    | 95/149 [54:38<40:03, 44.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analcatdata_draft\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                  | 96/149 [55:08<35:37, 40.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collins\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                 | 97/149 [55:44<33:44, 38.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prnn_fglass\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                               | 98/149 [56:10<29:59, 35.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jEdit_4.2_4.3\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                             | 99/149 [56:46<29:29, 35.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "mc2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                            | 100/149 [57:12<26:37, 32.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mw1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                          | 101/149 [57:44<25:56, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jEdit_4.0_4.2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                        | 102/149 [58:12<24:24, 31.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PopularKids\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                       | 103/149 [58:49<25:04, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teachingAssistant\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                     | 104/149 [59:21<24:27, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "lungcancer_GSE31210\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                    | 105/149 [59:49<22:55, 31.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MegaWatt1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                  | 106/149 [1:00:16<21:27, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PizzaCutter1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                | 107/149 [1:00:58<23:22, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PizzaCutter3\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                               | 108/149 [1:01:54<27:31, 40.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CostaMadre1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                             | 109/149 [1:02:38<27:30, 41.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CastMetal1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                            | 110/149 [1:03:06<24:24, 37.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnuggetChase3\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                          | 111/149 [1:03:31<21:24, 33.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PieChart1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                        | 112/149 [1:04:15<22:37, 36.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PieChart3\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 113/149 [1:05:05<24:26, 40.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n",
      "parkinsons\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                     | 114/149 [1:05:32<21:21, 36.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planning-relax\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 115/149 [1:05:56<18:34, 32.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qualitative-bankruptcy\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                  | 116/149 [1:06:22<16:58, 30.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa-heart\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                 | 117/149 [1:06:58<17:16, 32.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeds\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 118/149 [1:07:33<17:02, 32.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 79.10 GiB total capacity; 107.83 MiB already allocated; 54.56 MiB free; 116.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "thoracic-surgery\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                              | 119/149 [1:08:09<17:04, 34.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user-knowledge\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 120/149 [1:08:42<16:13, 33.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wholesale-customers\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 121/149 [1:09:16<15:44, 33.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart-long-beach\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                         | 122/149 [1:09:42<14:12, 31.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot-failures-lp5\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                        | 123/149 [1:10:16<13:53, 32.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "vertebra-column\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                      | 124/149 [1:10:47<13:13, 31.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartphone-Based_Recognition_of_Human_Activities\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                     | 125/149 [1:11:11<11:49, 29.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast-cancer-dropped-missing-attributes-values\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                   | 126/149 [1:11:39<11:08, 29.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LED-display-domain-7digit\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                 | 127/149 [1:12:17<11:39, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMETES_Epistasis_2-Way_20atts_0.1H_EDM-1_1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_ht\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_shallow\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_big\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "tabpfn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 128/149 [1:13:38<16:19, 46.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "calendarDOW\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_ht\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_shallow\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "distilled_tabpfn_big\n",
      "('The number of features for this classifier is restricted to ', 100)\n",
      "tabpfn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                              | 129/149 [1:14:34<16:26, 49.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of features for this classifier is restricted to ', 100)\n",
      "corral\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 130/149 [1:14:59<13:20, 42.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mofn-3-7-10\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_ht\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_shallow\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "distilled_tabpfn_big\n",
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "tabpfn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                           | 131/149 [1:16:06<14:52, 49.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ WARNING: TabPFN is not made for datasets with a trainingsize > 1024. Prediction might take a while, be less reliable. We advise not to run datasets > 10k samples, which might lead to your machine crashing (due to quadratic memory scaling of TabPFN). Please confirm you want to run by passing overwrite_warning=True to the fit function.\n",
      "thyroid-new\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 132/149 [1:16:34<12:12, 43.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solar-flare\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 133/149 [1:17:02<10:17, 38.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threeOf9\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                       | 134/149 [1:17:38<09:25, 37.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xd6\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 135/149 [1:18:31<09:51, 42.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokyo1\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 79.10 GiB total capacity; 321.79 MiB already allocated; 47.56 MiB free; 348.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 136/149 [1:19:37<10:41, 49.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity5_plus_5\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 137/149 [1:20:33<10:17, 51.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleve\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 138/149 [1:21:02<08:12, 44.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleveland-nominal\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 139/149 [1:21:32<06:41, 40.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australian\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 140/149 [1:22:13<06:05, 40.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "DiabeticMellitus\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 141/149 [1:22:44<05:00, 37.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conference_attendance\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 142/149 [1:23:10<03:59, 34.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPMP-2015-runtime-classification\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 143/149 [1:23:47<03:29, 34.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TuningSVMs\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 144/149 [1:24:12<02:40, 32.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regime_alimentaire\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 145/149 [1:24:46<02:10, 32.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.10 GiB total capacity; 8.02 MiB already allocated; 52.56 MiB free; 22.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "iris-example\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 146/149 [1:25:12<01:31, 30.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Touch2\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 147/149 [1:25:39<00:58, 29.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penguins\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 148/149 [1:26:10<00:30, 30.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic\n",
      "mlp_shallow\n",
      "mlp_big\n",
      "distilled_tabpfn_shallow\n",
      "distilled_tabpfn_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 149/149 [1:27:02<00:00, 35.05s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "for ds_name, X, y, categorical_features, _, _ in tqdm(cc_valid_datasets_multiclass):\n",
    "    print(ds_name)\n",
    "    for model_name, model_creator in models.items():\n",
    "        if not np.isnan(results.loc[ds_name, model_name]):\n",
    "            continue\n",
    "        print(model_name)\n",
    "        clf = model_creator(categorical_features)\n",
    "        if X.shape[1] > 100:\n",
    "            X = X[:, :100]\n",
    "        try:\n",
    "            scores = cross_validate(clf, X, y, scoring=\"roc_auc_ovo\", error_score=\"raise\")\n",
    "            score = scores['test_score'].mean()\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            score = np.NaN\n",
    "        results.loc[ds_name, model_name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b51ba4-1075-4512-bb7d-4ea1d0d07df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7901a-639e-4cb7-bf75-68f2abf62a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
